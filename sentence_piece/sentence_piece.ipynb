{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=kyoto-train.ja --model_prefix=kyoto-train-out.ja --vocab_size=15000 --hard_vocab_limit=false\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: kyoto-train.ja\n",
      "  input_format: \n",
      "  model_prefix: kyoto-train-out.ja\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 15000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: kyoto-train.ja\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 100000 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=3852540\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=3765\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 100000 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 693894 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 100000\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 98537\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 98537 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=297600 obj=142.496 num_tokens=1357945 num_tokens/piece=4.56299\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=264978 obj=129.022 num_tokens=1367971 num_tokens/piece=5.16258\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=197958 obj=130.403 num_tokens=1431311 num_tokens/piece=7.23038\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=196797 obj=129.374 num_tokens=1433517 num_tokens/piece=7.28424\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=147392 obj=132.545 num_tokens=1502264 num_tokens/piece=10.1923\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=147234 obj=131.565 num_tokens=1503015 num_tokens/piece=10.2083\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=110405 obj=135.487 num_tokens=1580544 num_tokens/piece=14.3159\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=110369 obj=134.479 num_tokens=1581179 num_tokens/piece=14.3263\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=82770 obj=138.815 num_tokens=1653907 num_tokens/piece=19.982\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=82766 obj=137.939 num_tokens=1654337 num_tokens/piece=19.9881\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=62073 obj=142.468 num_tokens=1731761 num_tokens/piece=27.8988\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=62071 obj=141.639 num_tokens=1732017 num_tokens/piece=27.9038\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46553 obj=146.377 num_tokens=1809815 num_tokens/piece=38.8764\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46553 obj=145.609 num_tokens=1810306 num_tokens/piece=38.887\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34914 obj=150.596 num_tokens=1891631 num_tokens/piece=54.1797\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34914 obj=149.826 num_tokens=1892037 num_tokens/piece=54.1914\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=26185 obj=154.994 num_tokens=1977674 num_tokens/piece=75.527\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26185 obj=154.175 num_tokens=1977911 num_tokens/piece=75.536\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19638 obj=159.572 num_tokens=2069638 num_tokens/piece=105.389\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19638 obj=158.669 num_tokens=2069849 num_tokens/piece=105.4\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=16500 obj=162.096 num_tokens=2128905 num_tokens/piece=129.025\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=16500 obj=161.507 num_tokens=2129006 num_tokens/piece=129.031\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: kyoto-train-out.ja.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: kyoto-train-out.ja.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "input_file=\"kyoto-train.ja\"\n",
    "output_file=\"kyoto-train-out.ja\"\n",
    "vocab_size=15000\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input=' + input_file + ' --model_prefix=' + output_file + ' --vocab_size=' + str(vocab_size) + ' --hard_vocab_limit=false'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
