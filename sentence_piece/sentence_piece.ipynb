{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=kyoto-train.ja --model_prefix=kyoto-train-out.ja --vocab_size=30000 --hard_vocab_limit=true\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: kyoto-train.ja\n",
      "  input_format: \n",
      "  model_prefix: kyoto-train-out.ja\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 30000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: kyoto-train.ja\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 100000 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=3852540\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=3765\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 100000 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 693894 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 100000\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 98537\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 98537 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=297600 obj=142.496 num_tokens=1357945 num_tokens/piece=4.56299\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=264978 obj=129.022 num_tokens=1367971 num_tokens/piece=5.16258\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=197958 obj=130.403 num_tokens=1431311 num_tokens/piece=7.23038\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=196797 obj=129.374 num_tokens=1433517 num_tokens/piece=7.28424\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=147392 obj=132.545 num_tokens=1502264 num_tokens/piece=10.1923\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=147234 obj=131.565 num_tokens=1503015 num_tokens/piece=10.2083\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=110405 obj=135.487 num_tokens=1580544 num_tokens/piece=14.3159\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=110369 obj=134.479 num_tokens=1581179 num_tokens/piece=14.3263\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=82770 obj=138.815 num_tokens=1653907 num_tokens/piece=19.982\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=82766 obj=137.939 num_tokens=1654337 num_tokens/piece=19.9881\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=62073 obj=142.468 num_tokens=1731761 num_tokens/piece=27.8988\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=62071 obj=141.639 num_tokens=1732017 num_tokens/piece=27.9038\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46553 obj=146.377 num_tokens=1809815 num_tokens/piece=38.8764\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46553 obj=145.609 num_tokens=1810306 num_tokens/piece=38.887\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34914 obj=150.596 num_tokens=1891631 num_tokens/piece=54.1797\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34914 obj=149.826 num_tokens=1892037 num_tokens/piece=54.1914\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=33000 obj=150.79 num_tokens=1909115 num_tokens/piece=57.852\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=33000 obj=150.627 num_tokens=1909159 num_tokens/piece=57.8533\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: kyoto-train-out.ja.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: kyoto-train-out.ja.vocab\n"
     ]
    }
   ],
   "source": [
    "input_file=\"kyoto-train.ja\"\n",
    "output_file=\"kyoto-train-out.ja\"\n",
    "vocab_size=30000\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input=' + input_file + ' --model_prefix=' + output_file + ' --vocab_size=' + str(vocab_size) + ' --hard_vocab_limit=true'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=test.txt --model_prefix=test.model --vocab_size=3000 --hard_vocab_limit=false --character_coverage=1.0\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: test.txt\n",
      "  input_format: \n",
      "  model_prefix: test.model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 3000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: test.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 2312 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=294227\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=174\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 2312 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 13757 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 2312\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 6820\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 6820 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=5885 obj=11.357 num_tokens=14241 num_tokens/piece=2.41988\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=4877 obj=9.6118 num_tokens=14426 num_tokens/piece=2.95797\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=3655 obj=9.60285 num_tokens=15531 num_tokens/piece=4.24925\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=3653 obj=9.53084 num_tokens=15570 num_tokens/piece=4.26225\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=3299 obj=9.58928 num_tokens=16161 num_tokens/piece=4.89876\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=3298 obj=9.56545 num_tokens=16160 num_tokens/piece=4.89994\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: test.model.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: test.model.vocab\n"
     ]
    }
   ],
   "source": [
    "input_file=\"test.txt\"\n",
    "model_file=\"test.model\"\n",
    "vocab_size=3000\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input=' + input_file + ' --model_prefix=' + model_file + ' --vocab_size=' + str(vocab_size) + ' --hard_vocab_limit=false'+ ' --character_coverage=1.0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file=\"test.txt\"\n",
    "output_file = \"test_out.txt\"\n",
    "model = \"test.model.model\"\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "doc_parsed = []\n",
    "with open(input_file, 'r', encoding='utf-8') as tf:\n",
    "    for line in tf:\n",
    "        pieces = \" \".join(sp.EncodeAsPieces(line))\n",
    "        doc_parsed.append(pieces)\n",
    "with open(output_file, 'w', encoding='utf8') as f:\n",
    "    for line in doc_parsed:\n",
    "        f.write(line + '\\n')\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 15, 52, 66, 272, 97, 27, 4, 0, 6, 4, 0, 6, 44, 4, 0, 8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.Encode(\"it is also written as 木華佐久耶姫 , 木花之佐久夜毘売 , or 木花開耶姫 .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
